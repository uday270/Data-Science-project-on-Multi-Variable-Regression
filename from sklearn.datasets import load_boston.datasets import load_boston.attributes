from sklearn.datasets import load_boston
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
import warnings
warnings.simplefilter('ignore')
boston_dataset = load_boston()
boston_dataset
dir(boston_dataset)
print(boston_dataset.DESCR)
boston_dataset.data.shape
data = pd.DataFrame(data = boston_dataset.data, columns = boston_dataset.feature_names)
data['PRICE'] = boston_dataset.target
pd.isnull(data)
pd.isnull(data).any()
plt.hist(data['PRICE'])
plt.show()
plt.figure(figsize= (10,6))
sns.distplot(data['PRICE'], color = 'red', bins = 50)
plt.show()
data['RM'].mean()
data['RAD'].value_counts()
accessibility = data['RAD'].value_counts()
accessibility.index
plt.figure(figsize= (10,6))
plt.bar(accessibility.index, accessibility )
plt.show()
data['CHAS'].value_counts()
plt.figure(figsize= (10,6))
plt.hist(data['CHAS'], ec = 'black', color = 'YELLOW', alpha = 0.6)
plt.xlabel('NEAR THE RIVER')
plt.ylabel('Nr of Houses')
plt.show()
data.max()
data.min()
data.mean()
data.median()
data['PRICE'].corr(data['RM'])
data.corr()
masker = np.zeros_like(data.corr())
triangle_indices = np.triu_indices_from(masker)
masker[triangle_indices] = True
plt.figure(figsize =(16,10))
sns.heatmap(data.corr())
plt.show()
plt.figure(figsize =(10,8))
sns.heatmap(data.corr(), mask = masker)
plt.xticks(fontsize = 10)
plt.yticks(fontsize = 10)
plt.show()
nox_dis_corr=round(data['NOX'].corr(data['DIS']),3)
nox_dis_corr
sns.set()
sns.set_style('dark')
sns.jointplot(x=data['DIS'], y =data['NOX'], color = 'red',height=8,ratio= 12, joint_kws = {'alpha' : 0.5},)
plt.show()
sns.set()
sns.set_style('dark')
sns.jointplot(x=data['DIS'], y =data['NOX'], kind='hex' )
plt.show()
sns.set()
sns.lmplot(x = 'RM', y = 'PRICE', data = data, height = 10, aspect = 2)
plt.show()
regr = LinearRegression()
regr.fit(X_train, y_train)
pd.DataFrame(data=regr.coef_, index=X_train.columns, columns=['coef'])
print('Intercept', regr.intercept_)
print('Training data r-squared:', regr.score(X_train, y_train))
print('Test data r-squared:', regr.score(X_test, y_test))
data['PRICE'].skew()
sns.set()
plt.figure(figsize=(20,6))
sns.distplot(y_log)
plt.title(f'Log price: The skew is {round(y_log.skew(),2)}', fontsize = 25)
plt.xlabel('LOG PRICES ', fontsize =14, color = 'green' )
plt.show()
skew=round(data['PRICE'].skew(),2)
sns.set()
plt.figure(figsize=(20,6))
sns.distplot(data['PRICE'])
plt.title(f'Actual price: The skew is {skew}', fontsize = 25)
plt.xlabel( 'ACTUAL PRICES ', fontsize =14, color = 'green' )
plt.show()
prices = np.log(data['PRICE'])
features = data.drop('PRICE', axis=1)
X_train, X_test, y_train, y_test = train_test_split(features, prices, 
                                                    test_size=0.2, random_state=10)
regr = LinearRegression()
regr.fit(X_train, y_train)
print('Training data r-squared:', regr.score(X_train, y_train))
print('Test data r-squared:', regr.score(X_test, y_test))
pd.DataFrame(data=regr.coef_, index=X_train.columns, columns=['coef'])
results=sm.OLS(y_train, sm.add_constant(X_train)).fit()
print(results.params)
print(results.pvalues)
for i in range(len(X_incl_const.columns)):    
print(variance_inflation_factor(exog = sm.add_constant(X_train).values, exog_idx=i), '\n')
X_incl_const = sm.add_constant(X_train) 
model = sm.OLS(y_train, X_incl_const) 
results_1 = model.fit()
org_coef = pd.DataFrame({'coef': results_1.params, 'p-value': round(results_1.pvalues, 3)})
print('BIC with all features and the log price is      :' , results_1.bic, '\n')
print('r-squared is with all features and log price is :' , results_1.rsquared, '\n')
print(org_coef)
X_incl_const = sm.add_constant(X_train)
X_incl_const = X_incl_const.drop(['INDUS','AGE'], axis =1)
model = sm.OLS(y_train, X_incl_const)
results_3 = model.fit()
coef_minus_indus_age = pd.DataFrame({'coef': results_3.params, 'p-value': round(results_3.pvalues, 3)})
print('BIC without INDUS and AGE features and the log price is      :' , results_3.bic, '\n')
print('r-squared  without INDUS and AGE features and log price is   :' , results_3.rsquared,'\n')
print(coef_minus_indus_age)
prices = np.log(data['PRICE'])
features = data.drop(['PRICE', 'AGE', 'INDUS'], axis = 1)
X_train, X_test, y_train, y_test = train_test_split(features,prices, test_size = 0.2, random_state = 10)
results = sm.OLS(y_train, sm.add_constant(X_train)).fit()
corr = round(y_train.corr(results.fittedvalues), 2)
plt.figure(figsize=(20,6))
plt.scatter(x=results.fittedvalues, y = results.resid, c='navy', alpha=0.6)
plt.xlabel('Predicted log prices $\hat y _i$', fontsize=20)
plt.ylabel('Residuals', fontsize=20)
plt.title('Residuals vs Fitted Values', fontsize=20)
plt.show()
reduced_log_mse = round(results.mse_resid, 3)
reduced_log_rsquared = round(results.rsquared, 3)
resid_mean = round(results.resid.mean(), 3)
resid_skew = round(results.resid.skew(), 3)
plt.figure(figsize= (20,6))
sns.distplot(results.resid, color='navy')# using the distplot of seaborn
plt.title(f'Log price model: residuals Skew ({resid_skew}) Mean ({resid_mean})')
plt.show()